[{"content":"Learning to Code with ChatGPT During the holidays I have had some down time and I have decided to dive deeper into my journey of learning to program. My normal way of learning if by building projects and just playing around, so that’s exactly what I have been doing but this time around I have had some extra help.\nI am currently building a platform where I can log transactions so I can keep track of what’s going on and how much has been spent, I am doing this as I don’t really like the current software that is out there, it’s too bulky and cluttered, so I decided to build my own with the help of ChatGPT.\nOh and just a side note, I have no idea how to use Django.\nThe first question I asked was;\nI am wanting to build a dashboard in Python Django to track american express transactions, what model will I need? A model in Django is like a way to declaring data types (which chatGPT taught me) and ChatGPT then proceeded to give me the results I was after.\n!(image)[https://imgur.com/a/Un3zEbb]\nChatGPT then proceeded to walk me through the steps required and bit by bit my project was being built, but I am also learning while doing so, if I don’t understand why something is done the way it is, I ask and ChatGPT will let me know.\nSo far it has allowed me to build a platform I was after, below are the results so far.\nHomepage !(image)[https://imgur.com/a/5IDEbkO]\nTransactions !(image)[https://imgur.com/a/n76KP3k]\nCreate Transaction !(image)[https://imgur.com/a/B3UNuql]\nAll of the above code was all created by ChatGPT and I was just the one throwing the ideas at it. Each time I got an idea, like “Hey, can you add a delete button next to each transaction, it would then go off and tell me how to do so.\nThe best thing was, I didn’t need to search around on Google if I got stuck, I just asked ChatGPT directly, and then I was guided on how to fix it.\n!(image)[https://imgur.com/a/5eMsy2C]\nThis is really awesome and has been a great way for me to learn and jump right into the deep end, instead of reading documentation on documentation before doing anything. #blog\n","permalink":"https://techdox.nz/posts/learning-to-code-with-chatgpt/","summary":"Learning to Code with ChatGPT During the holidays I have had some down time and I have decided to dive deeper into my journey of learning to program. My normal way of learning if by building projects and just playing around, so that’s exactly what I have been doing but this time around I have had some extra help.\nI am currently building a platform where I can log transactions so I can keep track of what’s going on and how much has been spent, I am doing this as I don’t really like the current software that is out there, it’s too bulky and cluttered, so I decided to build my own with the help of ChatGPT.","title":"Learning to Code With ChatGPT"},{"content":"DevSecOps I was very fortunate enough to be given a pass to go to the Native Cloud Summit here in Wellington by my work and it was an amazing two days, it allowed me to get an idea of what was happening in the industry and where my thinking and others within my teams thinking aligned with the industry and it allowed me to see that we are currently all facing the same issue, Security and DevOps.\nYou would have heard the term DevOps thrown around left and right if you work in the cloud and rightly so, it’s becoming the standard for how we design and deploy our cloud environments as well as setting organisation standards, but it seems the industry has an issue with working in where security comes into play, so I thought I would write up my thoughts on this topic.\nThere’s a common theme in the Cloud space where a project seems to be going along smoothly and things are being deployed to Dev and Test environments without issues and then it comes to deploying to Production and then all the wheels on the projects come to a screaming halt… Security has just been bought in and they are not happy, and they give you a report on their finding of all the things that need to be fixed before your production release can go live…. Say goodbye to that deadline target.\nThere needs to be a mind shift in the industry where security needs to be seen as much as a project as everyone else who is bought in at the start, security needs to be in the design talks, the build talks and deployment/handover talks so that there is no longer this “Surprised Pikachu face” when they give you the big NO for the production deployment build.\nWe tend to view security as a blocker or at least a pain in our backside when it comes to projects, but if we take a step back and see what point we bring them into a project, I think we can see where the problem is.\nNow, I am not saying every project is like this, but again there seems to be a pretty big gap between Devs and engineers working with security teams on projects and I may be preaching to the choir here, but I guess the point of this blog post is about changing our view of DevOps to be more DevSecOps.\nEverything we do, we do we should do it with security in mind from day one, be that we are implementing secure pipeline tools such as SNYK that scan our code and prevent vulnerabilities from entering our builds from day one to having our security team in with us during the very early discussions.\nTyping this out, it seems so logical that it doesn’t need to be spoken about but again, I was very surprised how many of us out there has this issue, and it’s time for a mental thinking change.\nTL: DR DevOps rarely involves security teams during the initial phases of a project where that thinking needs a massive mind shift where they are involved throughout the entire process, just as much as anything else. I see this solving a lot of issues with project deployments.\n","permalink":"https://techdox.nz/posts/devsecops/","summary":"DevSecOps I was very fortunate enough to be given a pass to go to the Native Cloud Summit here in Wellington by my work and it was an amazing two days, it allowed me to get an idea of what was happening in the industry and where my thinking and others within my teams thinking aligned with the industry and it allowed me to see that we are currently all facing the same issue, Security and DevOps.","title":"DevSecOps"},{"content":"Self Learning Via Projects When I am learning something new I always try and create a project of it and try aim it around a real world scenario, doing this allows me to understand it a lot more compared to just reading about it.\nThe reason I am writing a blog about this is to hopefully help those who are trying to get into a Cloud Engineering/DevOps role and to discuss what I have learned so far and what has helped me in my IT career.\nWhen you start to learn Cloud computing and everything it has to offer, nearly the first thing you will stumble upon are certifications. Certifications are something you should be looking to gain when trying to get into this space, as well as for those who are already in this space as it keeps you up to date with what services are being released and how they work while also testing your knowledge, there does seem to be some sort of misconception that I see often though and that’s that gaining a certification without any real world experience is enough.\nCertifications, again are a great way to demonstrate your understanding on how a providers services work and where to use what, but they do not give you the real word experience you get when actually deploying these services yourself outside of a controlled environment, this brings me to my main point.\nIf you are learning Cloud computing and trying to get your foot in the door somewhere, I highly suggest to make your own projects based on your learning outside of that controlled environment, most cloud providers give you a bit of free credit when first signing up and allow you to start deploying and playing around pretty quickly.\nCreating something as simple as a Linux server, running an Apache web server is a great place to start as it gives you a nice range of experience from;\nDeploying and configuring a Virtual Machine and Data Disks Managing and tracking spend Setting up logging and analytics Setting up user IAM permissions Setting up and managing secrets Managing network configurations and security …. As you can see above just deploying something simple gives you a great starting place to actually get your hands dirty and play around with the overall management of a cloud environment as during these controlled learning environments you don’t really need to think about this stuff unless it’s the specific learning objective.\nTLDR; Cloud certifications are great and will help you to learn the necessary fundamentals but real world practice is something you can’t avoid and creating your own personal projects is a great way to get some of that experience.\n","permalink":"https://techdox.nz/posts/personal-projects/","summary":"Self Learning Via Projects When I am learning something new I always try and create a project of it and try aim it around a real world scenario, doing this allows me to understand it a lot more compared to just reading about it.\nThe reason I am writing a blog about this is to hopefully help those who are trying to get into a Cloud Engineering/DevOps role and to discuss what I have learned so far and what has helped me in my IT career.","title":"Personal Projects and Cloud Certifications"},{"content":"When it comes to deploying Terraform via Pipelines your choices are almost limitless, you can go about it in so many ways, I personally have been deploying a lot of work based projects via Azure DevOps which has led me to also use it for personal projects.\nPipeline Setup The pipeline setup I use is pretty basic and I won\u0026rsquo;t go into the weeds of the yaml file etc, but I will cover what it\u0026rsquo;s doing and how I deploy it.\nThe Process So as you can see from the diagram above, it\u0026rsquo;s a rather simple concept. Work is done on the feature branch then pushed to Azure DevOps, the Terraform pipeline it run to check it\u0026rsquo;s correct and working and then a PR is raised to merge it into the main branch.\nWhen this PR is raised, the Infracost pipeline is triggered which this will show you the esitmated cost changes\nOnce the PR is approved, the full pipeline can be deployed, which looks like this.\nFinal notes This is a pretty high level view on how I am deploying Terraform + Infracost with Azure DevOps and I aim to do a walkthrough on the actual pipeline files, but I\u0026rsquo;m looking to make it a bit more simple first before I do that.\n","permalink":"https://techdox.nz/posts/devops-pipeline/","summary":"When it comes to deploying Terraform via Pipelines your choices are almost limitless, you can go about it in so many ways, I personally have been deploying a lot of work based projects via Azure DevOps which has led me to also use it for personal projects.\nPipeline Setup The pipeline setup I use is pretty basic and I won\u0026rsquo;t go into the weeds of the yaml file etc, but I will cover what it\u0026rsquo;s doing and how I deploy it.","title":"Devops Pipeline"},{"content":"This post is mainly a post for myself so that I remember how I structure Terraform for when I am deploying it in Azure DevOps.\nFile Structure main.tf providers.tf backend.tf variables.tf variables.tfvars modules main.tf The main file just contains your terraform resource and module blocks, not much needs to be explained here.\nproviders.tf Here is where we can set the version of terraform if we want to ensure that only a certain version of Terraform is used.\nAzure and Random are two other providers I am using since I am deploying to Azure and I am using random for generating my passwords that are stored in Keyvault, other providers such as AWS and Google can also be used in one file which would allow you to deploy to AWS and Azure incase you are wanting to create a VPN between them or something.\nterraform { required_providers { azurerm = { source = \u0026quot;hashicorp/azurerm\u0026quot; version = \u0026quot;~\u0026gt; 2.94.0\u0026quot; } random = { source = \u0026quot;hashicorp/random\u0026quot; version = \u0026quot;3.1.0\u0026quot; } } } provider \u0026quot;random\u0026quot; { # Configuration options } # Configure the Microsoft Azure Provider provider \u0026quot;azurerm\u0026quot; { features {} } backend.tf My backend is in Azure as is being stored in a Storage Account.\nterraform { backend \u0026quot;azurerm\u0026quot; { resource_group_name = \u0026quot;terraformState\u0026quot; #Resource group name storage_account_name = \u0026quot;pocterraformstate\u0026quot; #Storage account name container_name = \u0026quot;tfstate\u0026quot; #Container name in the storage account key = \u0026quot;platformstate\u0026quot; #File name of the state file } } variables.tf and tfvars These files are basic terraform files that I don\u0026rsquo;t think need to be explained, but I use variables.tf to set my variables with any descriptions they need and the tfvars to assign the value.\nThe use of tfvars is not needed as you can assign the value of the variable in the variables.tf file but I like using tfvars as it most projects we are deploying to many environments like prod, test etc and this keeps me in a good habit.\nExample of variables.tf #------------------------------ Subscription Variables ------------------------------ variable \u0026quot;subscription\u0026quot; { description = \u0026quot;Name for Subscription\u0026quot; } #------------------------------ Subscription Variables End ------------------------------ #------------------------------ Platform Variables ------------------------------ variable \u0026quot;platform_resource\u0026quot; { type = map(object({ rg_name = string rg_location = string })) } Example of tfvars #Subscriptions subscription = \u0026quot;40ba55ea-xxxxxxx\u0026quot; platform_resource = { base_resources = { rg_name = \u0026quot;xxxx-xxx\u0026quot; rg_location = \u0026quot;Australia East\u0026quot; } } Module folder Here is where I keep all my modules, not again this is the way it\u0026rsquo;s done and if you are used to using Terraform this is pretty standard, but here it is.\nmodules platform main.tf outputs.tf variables.tf virtualMachine '' '' '' vnets '' '' '' I am planning to do a write up on how I deploy Terraform using Azure DevOps and pipelines and how all this ties in, so stay tuned.\n","permalink":"https://techdox.nz/posts/terraform-layout/","summary":"This post is mainly a post for myself so that I remember how I structure Terraform for when I am deploying it in Azure DevOps.\nFile Structure main.tf providers.tf backend.tf variables.tf variables.tfvars modules main.tf The main file just contains your terraform resource and module blocks, not much needs to be explained here.\nproviders.tf Here is where we can set the version of terraform if we want to ensure that only a certain version of Terraform is used.","title":"Terraform File Structure For Azure DevOps"}]